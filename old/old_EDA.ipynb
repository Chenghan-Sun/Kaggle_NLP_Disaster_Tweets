{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_path = \"../code/\"\n",
    "sys.path.insert(0, helper_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from New_MissingValue import MissingValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative path\n",
    "data_path = \"../data/\"\n",
    "data_file_list = [\"train.csv\",\"test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text file: 27486 rows; 4 columns\n",
      "Testing text file: 3535 rows; 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a3d0a7d5ad</td>\n",
       "      <td>Spent the entire morning in a meeting w/ a ven...</td>\n",
       "      <td>my boss was not happy w/ them. Lots of fun.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251b6a6766</td>\n",
       "      <td>Oh! Good idea about putting them on ice cream</td>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9e8d1ef1c</td>\n",
       "      <td>says good (or should i say bad?) afternoon!  h...</td>\n",
       "      <td>says good (or should i say bad?) afternoon!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14f087215</td>\n",
       "      <td>i dont think you can vote anymore! i tried</td>\n",
       "      <td>i dont think you can vote anymore!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bf7473b12d</td>\n",
       "      <td>haha better drunken tweeting you mean?</td>\n",
       "      <td>better</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  a3d0a7d5ad  Spent the entire morning in a meeting w/ a ven...   \n",
       "1  251b6a6766      Oh! Good idea about putting them on ice cream   \n",
       "2  c9e8d1ef1c  says good (or should i say bad?) afternoon!  h...   \n",
       "3  f14f087215         i dont think you can vote anymore! i tried   \n",
       "4  bf7473b12d             haha better drunken tweeting you mean?   \n",
       "\n",
       "                                 selected_text sentiment  \n",
       "0  my boss was not happy w/ them. Lots of fun.   neutral  \n",
       "1                                         Good  positive  \n",
       "2  says good (or should i say bad?) afternoon!   neutral  \n",
       "3           i dont think you can vote anymore!  negative  \n",
       "4                                       better  positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data_train = pd.read_csv(data_path + data_file_list[0])\n",
    "data_test = pd.read_csv(data_path + data_file_list[1])\n",
    "\n",
    "# data info\n",
    "print(f'Training text file: {data_train.shape[0]} rows; {data_train.shape[1]} columns')\n",
    "print(f'Testing text file: {data_test.shape[0]} rows; {data_test.shape[1]} columns')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In column\u001b[91m text\u001b[00m , we have\u001b[91m 1\u001b[00m missing values.\n",
      "In column\u001b[91m selected_text\u001b[00m , we have\u001b[91m 1\u001b[00m missing values.\n",
      "textID           fdb77c3752\n",
      "text                    NaN\n",
      "selected_text           NaN\n",
      "sentiment           neutral\n",
      "Name: 13133, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13133]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_finder_train = MissingValue(data_train)\n",
    "missing_value_finder_train.missing_value_summary(verbose=True)\n",
    "missing_value_finder_train.missing_value_enumerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop this missing row\n",
    "for obj in missing_value_finder_train.na_index:\n",
    "    data_train = data_train.drop([obj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_finder_test = MissingValue(data_test)\n",
    "missing_value_finder_test.missing_value_summary(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing value found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({},\n",
       " 0,\n",
       " textID           0\n",
       " text             0\n",
       " selected_text    0\n",
       " sentiment        0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_finder_test = MissingValue(data_train)\n",
    "missing_value_finder_test.missing_value_summary(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Missing value check finished "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=pd.get_dummies(data_train['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning sentiment information with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import pickle\n",
    "import sys\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a preprocessed CSV file and gives statistics\n",
    "# Writes the frequency distribution of words and bigrams\n",
    "# to pickle files.\n",
    "\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = {}\n",
    "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
    "    result['URLS'] = tweet.count('URL')\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
    "        'URL', '')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams\n",
    "\n",
    "\n",
    "def get_bigrams(tweet_words):\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in xrange(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def get_bigram_freqdist(bigrams):\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) != 2:\n",
    "    print 'Usage: python stats.py <preprocessed-CSV>'\n",
    "    exit()\n",
    "num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "all_words = []\n",
    "all_bigrams = []\n",
    "with open(sys.argv[1], 'r') as csv:\n",
    "    lines = csv.readlines()\n",
    "    num_tweets = len(lines)\n",
    "    for i, line in enumerate(lines):\n",
    "        t_id, if_pos, tweet = line.strip().split(',')\n",
    "        if_pos = int(if_pos)\n",
    "        if if_pos:\n",
    "            num_pos_tweets += 1\n",
    "        else:\n",
    "            num_neg_tweets += 1\n",
    "        result, words, bigrams = analyze_tweet(tweet)\n",
    "        num_mentions += result['MENTIONS']\n",
    "        max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "        num_pos_emojis += result['POS_EMOS']\n",
    "        num_neg_emojis += result['NEG_EMOS']\n",
    "        max_emojis = max(\n",
    "            max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "        num_urls += result['URLS']\n",
    "        max_urls = max(max_urls, result['URLS'])\n",
    "        num_words += result['WORDS']\n",
    "        min_words = min(min_words, result['WORDS'])\n",
    "        max_words = max(max_words, result['WORDS'])\n",
    "        all_words.extend(words)\n",
    "        num_bigrams += result['BIGRAMS']\n",
    "        all_bigrams.extend(bigrams)\n",
    "        write_status(i + 1, num_tweets)\n",
    "num_emojis = num_pos_emojis + num_neg_emojis\n",
    "unique_words = list(set(all_words))\n",
    "with open(sys.argv[1][:-4] + '-unique.txt', 'w') as uwf:\n",
    "    uwf.write('\\n'.join(unique_words))\n",
    "num_unique_words = len(unique_words)\n",
    "num_unique_bigrams = len(set(all_bigrams))\n",
    "print '\\nCalculating frequency distribution'\n",
    "# Unigrams\n",
    "freq_dist = FreqDist(all_words)\n",
    "pkl_file_name = sys.argv[1][:-4] + '-freqdist.pkl'\n",
    "with open(pkl_file_name, 'wb') as pkl_file:\n",
    "    pickle.dump(freq_dist, pkl_file)\n",
    "print 'Saved uni-frequency distribution to %s' % pkl_file_name\n",
    "# Bigrams\n",
    "bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "bi_pkl_file_name = sys.argv[1][:-4] + '-freqdist-bi.pkl'\n",
    "with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
    "    pickle.dump(bigram_freq_dist, pkl_file)\n",
    "print 'Saved bi-frequency distribution to %s' % bi_pkl_file_name\n",
    "print '\\n[Analysis Statistics]'\n",
    "print 'Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets)\n",
    "print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
    "print 'URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls)\n",
    "print 'Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis)\n",
    "print 'Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words)\n",
    "print 'Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
